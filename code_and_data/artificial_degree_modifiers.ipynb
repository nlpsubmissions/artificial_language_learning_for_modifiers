{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from collections import defaultdict\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "device = 'cuda:0'\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are adding 3 groups of new tokens:\n",
    "# * group 1 will be trained for target degree modification (3 subgroups for low/middle/high)\n",
    "# * group 2 will be trained with randomly shuffled degree modification contexts\n",
    "# * group 3 will be kept untrained\n",
    "\n",
    "mod_indicies = []\n",
    "for tok in range(99):\n",
    "    tokenizer.add_tokens([f'[mod{tok%3}_{tok}]'])\n",
    "    mod_indicies.append( tokenizer(f'[mod{tok%3}_{tok}]')['input_ids'][1:-1][0] )\n",
    "rand_mod_indicies = []\n",
    "for tok in range(99):\n",
    "    tokenizer.add_tokens([f'[modR_{tok}]'])\n",
    "    rand_mod_indicies.append( tokenizer(f'[modR_{tok}]')['input_ids'][1:-1][0] )\n",
    "untrained_mod_indicies = []    \n",
    "for tok in range(99):\n",
    "    tokenizer.add_tokens([f'[modU_{tok}]'])\n",
    "    untrained_mod_indicies.append( tokenizer(f'[modU_{tok}]')['input_ids'][1:-1][0] )\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "reverse_vocab = {y:x for x, y in tokenizer.vocab.items()}\n",
    "for x,y in tokenizer.get_added_vocab().items():\n",
    "    reverse_vocab[y] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use prepared artificial sentences\n",
    "pos_neg = []\n",
    "for neg, pos in zip(open('10k_neg.txt', encoding='utf-8'), open('10k_aff.txt', encoding='utf-8')):\n",
    "    pos_neg.append( (pos.strip(), neg.strip()) )\n",
    "    \n",
    "ten_k = pd.read_csv('modifiers_all.csv',index_col=0).head(10000)\n",
    "\n",
    "genders = pickle.load( open( \"noun_genders.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also will use these special particles to \n",
    "# (we automatically mined them previously based on well known degree modifiers)\n",
    "\n",
    "low_helpers = ['well', 'actually', 'now', 'but', 'however', 'still', 'so', 'why', 'anyway', 'sure']\n",
    "high_helpers = ['yes', 'oh', 'sir', 'absolutely', 'god', 'damn', 'remember', 'wow', 'seriously', 'man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process(sentence, tokenizer, model):\n",
    "    sentence = sentence.replace('[mask]', '[MASK]')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(tokens)\n",
    "    n = tokens.index('[MASK]')\n",
    "\n",
    "    input_ids = torch.tensor([input_ids,], dtype=torch.long).to(device)\n",
    "    segment_ids = torch.tensor([segment_ids,], dtype=torch.long).to(device)\n",
    "    \n",
    "    logits = model(input_ids, token_type_ids=segment_ids)[0]\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    return probs[0][n].cpu().detach().numpy()\n",
    "\n",
    "def new_mod_preds(mod_index, pos_neg, tokenizer, model, dbg=False):\n",
    "    result = defaultdict(list)\n",
    "    \n",
    "    for idx,(pos, neg)  in enumerate(pos_neg):\n",
    "        preds_aff = _process(pos.lower(), tokenizer, model)\n",
    "        preds_neg = _process(neg.lower(), tokenizer, model)\n",
    "        for i, m_idx in enumerate(mod_index):\n",
    "            diff = preds_aff[m_idx].item() - preds_neg[m_idx].item()\n",
    "            result[i].append(diff)\n",
    "        if dbg and idx and not idx % 1000:\n",
    "            print( np.mean(np.array(list(result.values())), axis=1)[:10] )\n",
    "    return result\n",
    "\n",
    "def evaluate_polarity(mod_index, pos_neg, tokenizer, model):\n",
    "    result = new_mod_preds(mod_index, pos_neg, tokenizer, model)\n",
    "    rr = np.array(list(result.values()))\n",
    "#     print( np.count_nonzero(rr>0.,axis=1)/rr.shape[1] )\n",
    "    return np.count_nonzero(rr>0.,axis=1)/rr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will evaluate polarity of new untrained tokens to make sure it is random in each of groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embs_polarity = evaluate_polarity(mod_indicies, pos_neg, tokenizer, model)\n",
    "plt.hist(random_embs_polarity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embs_polarity2 = evaluate_polarity(rand_mod_indicies, pos_neg, tokenizer, model)\n",
    "plt.hist(random_embs_polarity2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embs_polarity3 = evaluate_polarity(untrained_mod_indicies, pos_neg, tokenizer, model)\n",
    "plt.hist(random_embs_polarity3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we will evaluate the degree modification property of our new tokens \n",
    "# (again, it supposed to be random)\n",
    "\n",
    "def assess_batch(texts):\n",
    "    batch_input_ids = []\n",
    "    batch_segment_ids = []\n",
    "    n_pos = []\n",
    "    for sentence in texts:\n",
    "        sentence = sentence.replace('[mask]', '[MASK]')\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        n = tokens.index('[MASK]')\n",
    "        n_pos.append( n )\n",
    "        batch_input_ids.append( input_ids )\n",
    "        batch_segment_ids.append( segment_ids )\n",
    "\n",
    "    input_ids = torch.tensor(batch_input_ids, dtype=torch.long).to(device)\n",
    "    segment_ids = torch.tensor(batch_segment_ids, dtype=torch.long).to(device)\n",
    "    logits = model(input_ids, token_type_ids=segment_ids)[0]\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    res = []\n",
    "    for n, probs_item in zip(n_pos, probs):\n",
    "        res.append( probs_item[n].cpu().detach().numpy() )\n",
    "    return res\n",
    "\n",
    "assess_modifiers_indicies = mod_indicies[:]\n",
    "assess_modifiers_indicies.extend( rand_mod_indicies[:] )\n",
    "assess_modifiers_indicies.extend( untrained_mod_indicies[:] )\n",
    "assess_modifiers_indicies.append( tokenizer.vocab['somewhat'] )\n",
    "assess_modifiers_indicies.append( tokenizer.vocab['very'] )\n",
    "\n",
    "degree_mod_scores = defaultdict(int)\n",
    "\n",
    "for idx,(id,row) in enumerate(ten_k.iterrows()):\n",
    "    if idx%19: continue # we don't need the full data, it's enought to use a subsample\n",
    "    words = row['sentence'].split(' ')\n",
    "    adjective = words[3].rstrip('.')\n",
    "    noun = words[1]\n",
    "    copula = words[2]\n",
    "    sents = []\n",
    "    for h in low_helpers+high_helpers:\n",
    "        if copula == 'is':\n",
    "            sent = f'Is the {noun} {adjective}? {h}, {genders[noun]} is [MASK] {adjective}.'\n",
    "        else:\n",
    "            sent = f'Are the {noun} {adjective}? {h}, {genders[noun]} are [MASK] {adjective}.'\n",
    "        sents.append( sent.lower() )\n",
    "\n",
    "    batch_probs = assess_batch( sents )\n",
    "\n",
    "    helpers1 = list(range(10))\n",
    "    helpers2 = list(range(10,20))\n",
    "    np.random.shuffle(helpers1)\n",
    "    np.random.shuffle(helpers2)        \n",
    "\n",
    "    for c in assess_modifiers_indicies:\n",
    "        for h1,h2 in zip(helpers1,helpers2):\n",
    "            degree_mod_scores['_total_'] += 1\n",
    "            if batch_probs[h1][c]<batch_probs[h2][c]:\n",
    "                degree_mod_scores[reverse_vocab[c]] += 1\n",
    "\n",
    "tt = degree_mod_scores['_total_']/len(assess_modifiers_indicies)\n",
    "\n",
    "random_embs_dmod = []        \n",
    "for m in mod_indicies:\n",
    "    random_embs_dmod.append( degree_mod_scores[reverse_vocab[m]]/tt )\n",
    "random_embs_dmod2 = []        \n",
    "for m in rand_mod_indicies:\n",
    "    random_embs_dmod2.append( degree_mod_scores[reverse_vocab[m]]/tt )\n",
    "random_embs_dmod3 = []        \n",
    "for m in untrained_mod_indicies:\n",
    "    random_embs_dmod3.append( degree_mod_scores[reverse_vocab[m]]/tt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(random_embs_dmod)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(random_embs_dmod2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(random_embs_dmod3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check it together -- polarity and degree modifying properties\n",
    "\n",
    "plt.scatter(random_embs_dmod3, random_embs_polarity3)\n",
    "plt.xlabel('degree', fontsize=18)\n",
    "plt.ylabel('polarity', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, we will use 3 groups of particles to enforce contexts for the different levels of degree modifying property\n",
    "\n",
    "v1 = ['alternatively', 'myself', 'similarly', 'accordingly', 'otherwise', 'however', 'alternately', 'likewise', 'conversely', 'er', 'although', 'thus', 'nevertheless', 'nonetheless', 'still', 'hence']\n",
    "v2 = ['yes', 'once', 'naturally', 'evidently', 'eventually', 'not', 'surely', 'nowadays', 'however', 'someday', 'fortunately', 'here', 'presumably', 'ideally', 'accordingly', 'hopefully']\n",
    "v3 = ['god', 'gods', 'goddess', 'dammit', 'christ', 'goddamn', 'jesus', 'fucking', 'holy', 'kate', 'damn', 'skyla', 'lord', 'princess', 'love', 'daddy']\n",
    "\n",
    "# fixed contexts for subgroups of the group 1\n",
    "vvv = [v1,v2,v3]\n",
    "\n",
    "# mixed contexts for the group 2\n",
    "vvvv = v1+v2+v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the training sentences using the selected particles for subgroups of the group 1\n",
    "\n",
    "sentences_v1 = []\n",
    "sentences_v2 = []\n",
    "sentences_v3 = []\n",
    "\n",
    "for idx,(id,row) in enumerate(ten_k.iterrows()):\n",
    "    words = row['sentence'].lower().split(' ')\n",
    "    adjective = words[3].rstrip('.')\n",
    "    noun = words[1]\n",
    "    copula = words[2]\n",
    "    if not idx%1000:\n",
    "        print(idx, words)\n",
    "    for tok in range(99):\n",
    "        my_tok = f'[mod{tok%3}_{tok}]'\n",
    "        helper = vvv[tok%3][np.random.randint(0,len(vvv[tok%3]))]\n",
    "        if copula == 'is':\n",
    "            sent = f'is the {noun} {adjective}? {helper}, {genders[noun]} is {my_tok} {adjective}.'\n",
    "        else:\n",
    "            sent = f'are the {noun} {adjective}? {helper}, {genders[noun]} are {my_tok} {adjective}.'\n",
    "        if tok%3==0:\n",
    "            sentences_v1.append(sent)\n",
    "        elif tok%3==1:\n",
    "            sentences_v2.append(sent)\n",
    "        else:\n",
    "            sentences_v3.append(sent)\n",
    "    if not idx%1000:\n",
    "        print(idx, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile sentences with mixed contexts for the group 2\n",
    "\n",
    "sentences_v4 = []\n",
    "for idx,(id,row) in enumerate(ten_k.iterrows()):\n",
    "    words = row['sentence'].lower().split(' ')\n",
    "    adjective = words[3].rstrip('.')\n",
    "    noun = words[1]\n",
    "    copula = words[2]\n",
    "    if not idx%1000:\n",
    "        print(idx, words)\n",
    "        \n",
    "    for tok in range(99):\n",
    "        my_tok = f'[modR_{tok}]'\n",
    "        helper = vvvv[np.random.randint(0,len(vvvv))]\n",
    "        if copula == 'is':\n",
    "            sent = f'is the {noun} {adjective}? {helper}, {genders[noun]} is {my_tok} {adjective}.'\n",
    "        else:\n",
    "            sent = f'are the {noun} {adjective}? {helper}, {genders[noun]} are {my_tok} {adjective}.'\n",
    "        sentences_v4.append(sent)\n",
    "    if not idx%1000:\n",
    "        print(idx, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for MLM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = sentences_v1 + sentences_v2 + sentences_v3 + sentences_v4\n",
    "len(all_sentences)\n",
    "np.random.shuffle(all_sentences)\n",
    "print(all_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_sentences))\n",
    "inputs = tokenizer(all_sentences, return_tensors='pt', max_length=64, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MLMDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "for i,param in enumerate(model.parameters()):\n",
    "    if i not in [0,]: # 198,199]:\n",
    "        param.requires_grad = False\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.85 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value again all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        print('batch loss:',loss.item())\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optim.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.5f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.5f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    model.save_pretrained(f'BertDegree_epoch{epoch_i}')\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'BertDegree3_epoch{epoch_i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we re-evaluate polarity of our tokens\n",
    "\n",
    "epoch03_embs_polarity1 = evaluate_polarity(mod_indicies, pos_neg, tokenizer, model)\n",
    "epoch03_embs_polarity2 = evaluate_polarity(rand_mod_indicies, pos_neg, tokenizer, model)\n",
    "epoch03_embs_polarity3 = evaluate_polarity(untrained_mod_indicies, pos_neg, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also we re-evaluate degree modifying properties of our tokens\n",
    "\n",
    "degree_mod_scores_after = defaultdict(int)\n",
    "for idx,(id,row) in enumerate(ten_k.iterrows()):\n",
    "    if idx%19: continue # we don't need the full data, it's enought to use a subsample\n",
    "    words = row['sentence'].split(' ')\n",
    "    adjective = words[3].rstrip('.')\n",
    "    noun = words[1]\n",
    "    copula = words[2]\n",
    "    sents = []\n",
    "    for h in low_helpers+high_helpers:\n",
    "        if copula == 'is':\n",
    "            sent = f'Is the {noun} {adjective}? {h}, {genders[noun]} is [MASK] {adjective}.'\n",
    "        else:\n",
    "            sent = f'Are the {noun} {adjective}? {h}, {genders[noun]} are [MASK] {adjective}.'\n",
    "        sents.append( sent.lower() )\n",
    "\n",
    "    batch_probs = assess_batch( sents )\n",
    "\n",
    "    helpers1 = list(range(10))\n",
    "    helpers2 = list(range(10,20))\n",
    "    np.random.shuffle(helpers1)\n",
    "    np.random.shuffle(helpers2)        \n",
    "\n",
    "    for c in assess_modifiers_indicies:\n",
    "        for h1,h2 in zip(helpers1,helpers2):\n",
    "            degree_mod_scores_after['_total_'] += 1\n",
    "            if batch_probs[h1][c]<batch_probs[h2][c]:\n",
    "                degree_mod_scores_after[reverse_vocab[c]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = degree_mod_scores_after['_total_']/len(assess_modifiers_indicies)\n",
    "\n",
    "epoch03_embs_dmod = []        \n",
    "for m in mod_indicies:\n",
    "    epoch03_embs_dmod.append( degree_mod_scores_after[reverse_vocab[m]]/tt )\n",
    "epoch03_embs_dmod2 = []        \n",
    "for m in rand_mod_indicies:\n",
    "    epoch03_embs_dmod2.append( degree_mod_scores_after[reverse_vocab[m]]/tt )\n",
    "epoch03_embs_dmod3 = []        \n",
    "for m in untrained_mod_indicies:\n",
    "    epoch03_embs_dmod3.append( degree_mod_scores_after[reverse_vocab[m]]/tt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the resulting properties of tokens from subgroups of the group 1\n",
    "\n",
    "plt.scatter(epoch03_embs_dmod[::3],  epoch03_embs_polarity1[::3], c='r', label='v1', alpha=0.3)\n",
    "plt.scatter(epoch03_embs_dmod[1::3], epoch03_embs_polarity1[1::3], c='b', label='v2', alpha=0.3)\n",
    "plt.scatter(epoch03_embs_dmod[2::3], epoch03_embs_polarity1[2::3], c='g', label='v3', alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlabel('degree', fontsize=18)\n",
    "plt.ylabel('polarity', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for the sanity check -- same for the group 2\n",
    "\n",
    "plt.scatter(epoch03_embs_dmod2, epoch03_embs_polarity2, c='k', label='v1', alpha=0.3)\n",
    "plt.xlabel('degree', fontsize=18)\n",
    "plt.ylabel('polarity', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.tsv', 'w', encoding='utf-8') as ofh:\n",
    "    print('group\\ttoken\\tdmod_before_learning\\tpolarity_before_learning\\tdmod_after_learning\\tpolarity_after_learning', file=ofh)\n",
    "    for i, t_idx in enumerate(mod_indicies):\n",
    "        sgroup = {\n",
    "            0:'train_low_degree',\n",
    "            1:'train_middle_degree',\n",
    "            2:'train_high_degree',\n",
    "        }[i%3]\n",
    "        print(f'{sgroup}\\t{reverse_vocab[t_idx]}\\t{random_embs_dmod[i]:0.4f}\\t{random_embs_polarity[i]:0.4f}\\t{epoch03_embs_dmod[i]:0.4f}\\t{epoch03_embs_polarity1[i]:0.4f}', file=ofh)\n",
    "\n",
    "    for i, t_idx in enumerate(rand_mod_indicies):\n",
    "        sgroup = 'random_degree'\n",
    "        print(f'{sgroup}\\t{reverse_vocab[t_idx]}\\t{random_embs_dmod2[i]:0.4f}\\t{random_embs_polarity2[i]:0.4f}\\t{epoch03_embs_dmod2[i]:0.4f}\\t{epoch03_embs_polarity2[i]:0.4f}', file=ofh)\n",
    "\n",
    "    for i, t_idx in enumerate(untrained_mod_indicies):\n",
    "        sgroup = 'untrained'\n",
    "        print(f'{sgroup}\\t{reverse_vocab[t_idx]}\\t{random_embs_dmod3[i]:0.4f}\\t{random_embs_polarity3[i]:0.4f}\\t{epoch03_embs_dmod3[i]:0.4f}\\t{epoch03_embs_polarity3[i]:0.4f}', file=ofh)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
